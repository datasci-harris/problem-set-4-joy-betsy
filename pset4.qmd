---
title: "30538 Problem Set 4"
date: "Nov 3"
geometry: margin=1in
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = fa lse,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Betsy Shi, betsyshi
    - Partner 2 (name and cnet ID): Joy Wu, joywu
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: BS JW
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**" Betsy Shi & Joy Wu (1 point)
6. Late coins used this pset: 1 Late coins left after submission: 2
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import geopandas as gpd
import time
import random
from shapely.ops import nearest_points
```

## Download and explore the Provider of Services (POS) file (10 pts)

1.
Based on the rest of the problem set and the data dictionary, I pulled ZIP_CD, PGM_TRMNTN_CD, STATE_CD, PRVDR_NUM, FAC_NAME, CMPLNC_STUS_CD, CITY_NAME, PRVDR_CTGRY_CD, PRVDR_CTGRY_SBTYP_CD as variables.


2. 
    a.
```{python}
path = "data/pos2016.csv"
df_2016 = pd.read_csv(path)

short_term_hospitals = df_2016[(df_2016['PRVDR_CTGRY_CD'] == 1) 
                     & (df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]

num_short_term = short_term_hospitals.shape[0]
print(num_short_term)
```
Yes, based on the pos2016 data and definitions, the number 7,245 is logical and consistent with the datasetâ€™s criteria for short-term hospitals.


    b.
Cross-reference with 2024 AHA HOSPITAL STATISTICS. The number of U.S. community hospitals in 2024 is 5,129. (Reference: https://www.aha.org/statistics/fast-facts-us-hospitals)
The gap may come from definition difference. The community hospitals in the research are defined as all nonfederal, short-term general, and other special hospitals. This does not fit the short-term designation in pos2016 dataset.
And since 2016, there has been a trend of hospital closures, particularly in rural and less profitable areas. These closures could reduce the number of short-term facilities, meaning fewer hospitals are categorized as such in 2024.


3. 
```{python}
years = [2017, 2018, 2019]
data_frames = []

for year in years:
    
    file_path = f'data/pos{year}.csv'
    df = pd.read_csv(file_path, encoding='ISO-8859-1')
    df['year'] = year
    all_short_term = df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    data_frames.append(all_short_term)

data_all = pd.concat(data_frames, ignore_index=True)

short_term_hospitals['year'] = 2016
data_all = pd.concat([data_all, short_term_hospitals], ignore_index=True)

observation_by_year = data_all.groupby('year').size()
print(observation_by_year)

observation_by_year.plot(kind='bar')
plt.title('Number of Observations by Year')
plt.xlabel('Year')
plt.ylabel('Number of Observations')
plt.xticks(rotation=0)
plt.show()
```


4. 
    a.
```{python}
unique_hospital = data_all.groupby('year')['PRVDR_NUM'].nunique()

unique_hospital.plot(kind='bar', color='pink')
plt.title('Unique Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.xticks(rotation=0)
plt.show()
```


    b.
The two plots are highly identical, having similar count each year from 2016 to 2019 wich each bar being aroud 7,000.
This suggests that each hospital only has one record each year, with no extra records or repeated entries. This consistency shows that the dataset is well organized.
And since both the total number of observations an unique hospital counts are stable, there is no sigh of additional data collection periods within each year. This further supports that the deata is collected once a year.
The high consistency between the two plots over time suggests that the data has high integrity, with no missing data or big changes in participants.


## Identify hospital closures in POS file (15 pts) (*)

1.
```{python}
data_all['PRVDR_NUM'] = pd.to_numeric(data_all['PRVDR_NUM'], errors='coerce')

active_2016 = data_all[(data_all['year'] == 2016) & (data_all['PGM_TRMNTN_CD'] == 0)]
active_2016 = active_2016[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']]
original_list = active_2016['PRVDR_NUM'].unique()

closed_hospitals_list = []

for year in [2017, 2018, 2019]:
    
    active_year = data_all[(data_all['year'] == year) & (data_all['PGM_TRMNTN_CD'] == 0)]
    
    active_year = active_year[active_year['PRVDR_NUM'].isin(original_list)]
    active_list_year = active_year['PRVDR_NUM'].unique()
    
    closed_in_year = active_2016[~active_2016['PRVDR_NUM'].isin(active_list_year)]
    
    closed_in_year = closed_in_year.copy()
    closed_in_year['Suspected_Closure_Year'] = year
    closed_hospitals_list.append(closed_in_year)

    closed_list = closed_in_year['PRVDR_NUM'].unique()
    original_list = original_list[~np.isin(original_list, closed_list)]
    active_2016 = active_2016[active_2016['PRVDR_NUM'].isin(active_list_year)]


all_closed_hospitals = pd.concat(closed_hospitals_list, ignore_index=True)

num_closed = all_closed_hospitals.shape[0]
print(f'Number of hospitals suspected to have closed between 2016 and 2019: {num_closed}')
print(all_closed_hospitals[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].head())
```


2. 
```{python}
sorted_closed = all_closed_hospitals.sort_values(by='FAC_NAME')
top_10_closed = sorted_closed[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].head(10)
print(top_10_closed)
```


3. 
    a. & b.
```{python}
def identify_valid_closed_hospitals(df, list):
    valid_list = []
    potential_mergers = 0

    active_hospitals_count = (
    df[df['PGM_TRMNTN_CD'] == 0]
    .groupby(['year', 'ZIP_CD'])
    .size()
    .unstack(fill_value=0)
)

    for _, closed_hospital in list.iterrows():
        zip_code = closed_hospital['ZIP_CD']
        closure_year = closed_hospital['Suspected_Closure_Year']
        next_year = closure_year + 1

        if closure_year < 2019:
            current_year_count = active_hospitals_count.loc[closure_year, zip_code]
            next_year_count = active_hospitals_count.loc[next_year, zip_code]

            if current_year_count > next_year_count:
                valid_list.append(closed_hospital)
            else:
                potential_mergers += 1

        else:
            valid_list.append(closed_hospital)

    valid_closed_hospitals = pd.DataFrame(valid_list)
    num_valid_closed = valid_closed_hospitals.shape[0]

    return valid_closed_hospitals, num_valid_closed, potential_mergers

results = identify_valid_closed_hospitals(data_all, sorted_closed)

print(f'Number of potential mergers: {results[2]}')
print(f'Number of valid suspected hospital closures: {results[1]}')
print(results[0].head())
```
I also notice a case where both closure year and following year have zero active hospitals. If so, it does not indicate a potential merger but rather an absence of active hospitals in that area. In this case, the code will be different by adding "current_year_count > 0" as one more condition into the "if current_year_count > next_year_count:" part. So it would be, "if current_year_count > 0 and current_year_count > next_year_count:", and the result would be:

Number of potential mergers: 31;
Number of valid suspected hospital closures: 143.

I don't put this condition into my actual code part and get the result of merger 97, closure 77, which is also used in section5 plots, since the question focuses on whether there is a degree. This is also a limitation of the "first-pass" method mentioned in section6.


    c.
```{python}
valid_sorted_closed = results[0].sort_values(by='FAC_NAME')
top_10_closed = valid_sorted_closed[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].head(10)
print(top_10_closed)
```


## Download Census zip code shapefile (10 pt) 

1. 
    a.
.dbf is a database file which contains attribute data in a tabular format. Each record in it corresponds to a geographical feature.
.prj is a projection file which defines the coordinate system and projection information for the shapefile.
.shp is a shapefile which is the main file containing the geometry of each feature, including the spatial data that represent the shapes of geographical features.
.shx is a shape index file which provides an index of the geonmetry in the .shp file, helping to improve the loading of spatial data.
.xml is a metadata file which contains metadata and provides descriptive information about the dataset.


    b. 
.dbf: 6.4 MB
.prj: 165 bytes
.shp: 837.5 MB
.shx: 265 KB
.xml: 16 KB
As seen, the .shp file is the largest, the .shx file and .xml file are relatively small, and the .prj file is the smallest.


2. 
```{python}
filepath = "data/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
data_zip = gpd.read_file(filepath)
data_zip.head(100)

texas_prefixes = ('75', '76', '77', '78', '79')
zips_texas = data_zip[data_zip['ZCTA5'].str.startswith(texas_prefixes)]
    
estfile = 'data'
data = pd.read_csv(f'{estfile}/pos2016.csv')
hospitals_2016 = data[
    (data['PGM_TRMNTN_CD'] == 0) &
    (data['PRVDR_CTGRY_SBTYP_CD'] == 1) &
    (data['PRVDR_CTGRY_CD'] == 1)
].copy()

hospitals_2016['ZIP_CD'] = hospitals_2016['ZIP_CD'].astype(str).str[:5]
hospitals_2016['ZIP_CD'] = hospitals_2016['ZIP_CD'].str.zfill(5)
hospitals_texas = hospitals_2016[hospitals_2016['ZIP_CD'].str.startswith(texas_prefixes)].copy()

hospitals_per_zip = hospitals_texas.groupby('ZIP_CD')['PRVDR_NUM'].nunique().reset_index()
hospitals_per_zip.columns = ['ZIP_CD', 'hospital_count']

zips_texas = zips_texas.copy()
zips_texas = zips_texas.merge(hospitals_per_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left')
zips_texas['hospital_count'] = zips_texas['hospital_count'].fillna(0)

fig, ax = plt.subplots(figsize=(14, 14))
zips_texas.plot(
    column='hospital_count',
    cmap='Blues',
    linewidth=0.8,
    ax=ax,
    edgecolor='0.6',
    legend=True,
    legend_kwds={'label': "Number of Hospitals", 'orientation': "horizontal"}
)
ax.set_title('Number of Hospitals per Zip Code in Texas', fontsize=15)
ax.set_axis_off()
plt.show()
```


## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
data_zip['centroid'] = data_zip.geometry.centroid

zips_all_centroids = gpd.GeoDataFrame(data_zip[['ZCTA5', 'centroid']], geometry='centroid', crs=data_zip.crs)

print("Dimensions of zips_all_centroids:", zips_all_centroids.shape)
```
The resulting GeoDataFrame has 33,120 rows and 2 columns.
ZCTA5: it is a distinct ZIP code area in U.S.
Centroid: it is a geometry column that contains the geographic center point of the zip code area as a point geometry. Each point represents the central location within the boundaries of this row's zip code polygon.


2. 
```{python}
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(texas_prefixes)]
num_texas_zip = zips_texas_centroids['ZCTA5'].nunique()
print("The Number of unique zip codes in Texas:", num_texas_zip)

border_states_prefixes = texas_prefixes + ('70', '71', '72', '73', '74', '87', '88')
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(border_states_prefixes)]
num_borderstate_zip = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print("The Number of unique zip codes in Texas and bordering states:", num_borderstate_zip)
```


3. 
```{python}
hospital_boarderstates = hospitals_2016[hospitals_2016['ZIP_CD'].str.startswith(border_states_prefixes)]
hospitals_per_zip = hospital_boarderstates['ZIP_CD'].value_counts().reset_index()
hospitals_per_zip.columns = ['ZIP_CD', 'hospital_count']

hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(str).str.replace('.0', '', regex=False)
zips_texas_borderstates_centroids['ZCTA5'] = zips_texas_borderstates_centroids['ZCTA5'].astype(str).str.replace('.0', '', regex=False)

zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(hospitals_per_zip, left_on = 'ZCTA5', right_on = 'ZIP_CD', how = 'left').fillna(0)
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['hospital_count'] >= 1]

print(len(zips_withhospital_centroids))
print(zips_withhospital_centroids.head())
```
I'm using left merge.  
Merge Variables: The variables used in the merge are ZCTA5 (from zips_texas_borderstates_centroids) and ZIP_CD (from hospitals_per_zip).  


4. 
    a.
```{python}
subset_zips = zips_texas_centroids.sample(n=10, random_state=10)

start_time = time.time()

def cal_nearest_distance(row, df):
    nearest_geom = nearest_points(row['centroid'], df.unary_union)[1]
    return row['centroid'].distance(nearest_geom)

subset_zips['nearest_distance'] = subset_zips.apply(
    cal_nearest_distance,
    df=zips_withhospital_centroids,
    axis=1
)

end_time = time.time()

time_calculations = end_time - start_time
print(f"Time taken (10 zipcodes): {time_calculations} seconds")
time_all = time_calculations * (len(zips_texas_centroids)/10)
print(f"Time taken (all): {time_all} seconds")
```


    b.
```{python}
start_time_b = time.time()

zips_texas_centroids['nearest_distance'] = zips_texas_centroids.apply(
    cal_nearest_distance,
    df=zips_withhospital_centroids,
    axis=1
)

end_time_b = time.time()

time_calculations_b = end_time_b - start_time_b
print(f"Total time: {time_calculations_b} seconds")
```
The difference is significant, it actually runs much faster than I expected. 


    c.
UNIT["Degree",0.017453292519943295] indicates that the unit is degree (latitude and longitude coordinates), and the number 0.017453292519943295 is the factor that converts degree to radian. Transform the latitude and longitude into UTM using pyproj coordinates to accurately calculate the distance (in meters).  
1m â‰ˆ 0.000621371miles  
```{python}
zips_texas_centroids = zips_texas_centroids.to_crs(epsg=32614)
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=32614)

zips_texas_centroids['nearest_distance_meters'] = zips_texas_centroids.apply(
    cal_nearest_distance,
    df=zips_withhospital_centroids,
    axis=1
)

zips_texas_centroids['nearest_distance_miles'] = zips_texas_centroids['nearest_distance_meters'] * 0.000621371
print(zips_texas_centroids[['ZCTA5', 'nearest_distance_miles']].head())
```


5. 
    a.
I started with a distance calculation using UTM (EPSG:32614), which gives the units of meters. The meters were then converted to miles in order to report the average distance from each ZIP code to the nearest hospital.


    b.
```{python}
average_distance_meters = zips_texas_centroids['nearest_distance_meters'].mean()
average_distance_miles = average_distance_meters * 0.000621371
print(f"Avg distance to the nearest hospital: {average_distance_miles:.2f} miles")
```
The average distance of 13.43 miles to the nearest hospital may seem like a reasonable result since Texas is a sparsely populated state.  
In Texas, the distance between hospitals varies by region. According to the 2022 American Hospital Association report, the average Texas resident would have to travel approximately 8.5 miles to reach the nearest hospital. There is also data showing that the average rural Texas resident is about twice as far from a hospital as an urban resident, at about 12.5 miles, demonstrating the challenges of healthcare access in rural areas.  
Reference:  
https://www.pewresearch.org/short-reads/2018/12/12/how-far-americans-live-from-the-closest-hospital-differs-by-community-type/  
https://www.aha.org/news/headline/2018-12-14-study-rural-residents-travel-about-twice-far-hospital-average 


    c.
```{python}
zips_texas = zips_texas.merge(
    zips_texas_centroids[['ZCTA5', 'nearest_distance_miles']],
    on='ZCTA5',
    how='left'
)

fig, ax = plt.subplots(figsize=(14, 14))
zips_texas.plot(
    column='nearest_distance_miles',
    cmap='Blues',
    linewidth=0.8,
    ax=ax,
    edgecolor='0.6',
    legend=True
)
ax.set_title('Distance to Nearest Hospital in Texas', fontsize=15)
ax.set_axis_off()
plt.show()
```


## Effects of closures on access in Texas (15 pts)

1. 
```{python}
texas_closures = valid_sorted_closed[
    valid_sorted_closed['ZIP_CD'].astype(str).str.startswith(texas_prefixes)
]

texas_closures_by_zip = texas_closures.groupby('ZIP_CD').size().reset_index(name='closure_count')

texas_closures_by_zip['ZIP_CD'] = texas_closures_by_zip['ZIP_CD'].astype(str).str.split('.').str[0]

print(texas_closures_by_zip)
```


2. 
```{python}
zips_texas = zips_texas.merge(
    texas_closures_by_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left'
)
zips_texas['closure_count'] = zips_texas['closure_count'].fillna(0)
zips_texas = zips_texas.drop(columns=['ZIP_CD_x', 'ZIP_CD_y'])
print(zips_texas.head())

fig, ax = plt.subplots(figsize=(14, 14))
zips_texas.plot(
    column='closure_count',
    cmap='Blues',
    linewidth=0.8,
    ax=ax,
    edgecolor='0.6',
    legend=True,
    legend_kwds={'label': "Number of Closures", 'orientation': "horizontal"}
)
ax.set_title('Texas zip codes directly affected by a closure in 2016-2019', fontsize=15)
ax.set_axis_off()
plt.show()

directly_affected_zips = texas_closures_by_zip['ZIP_CD'].nunique()
print(f"Number of Directly Affected Zip Codes in Texas: {directly_affected_zips}")
```


3. 
```{python}
directly_affected = zips_texas.merge(
    texas_closures_by_zip, left_on='ZCTA5', right_on='ZIP_CD', how='inner'
)
print(directly_affected)

directly_affected = directly_affected.to_crs(epsg=32614)

buffer_meters = 10 / 0.000621371
directly_affected['buffer'] = directly_affected.geometry.buffer(buffer_meters)

indirectly_affected = gpd.sjoin(
    zips_texas.to_crs(epsg=32614),
    directly_affected.set_geometry('buffer'),
    how='inner',
    predicate='intersects'
)

indirectly_affected = indirectly_affected[
    ~indirectly_affected['ZCTA5_left'].isin(directly_affected['ZCTA5'])
]

indirectly_affected_count = indirectly_affected['ZCTA5_left'].nunique()
print(f"The number of directly affected zip codes in Texasï¼š{indirectly_affected_count}")
```


4. 
```{python}
zips_texas['impact_category'] = 'Not Affected'

zips_texas.loc[
    zips_texas['ZCTA5'].isin(directly_affected['ZCTA5']), 'impact_category'
] = 'Directly Affected'

zips_texas.loc[
    zips_texas['ZCTA5'].isin(indirectly_affected['ZCTA5_left']), 
    'impact_category'
] = "Indirectly Affected"

fig, ax = plt.subplots(figsize=(14, 14))
zips_texas.plot(
    column='impact_category',
    cmap='Set2',
    linewidth=0.1,
    ax=ax,
    edgecolor='0.6',
    legend=True,
    legend_kwds={'title': "Impact Category", 'fontsize': 'large', 'title_fontsize': '15'}
)
ax.set_title('Impact of Hospital Closures on Texas Zip Codes', fontsize=15)
ax.set_axis_off()
plt.show()
```


## Reflecting on the exercise (10 pts) 

Partner_1:

The "first-pass" method, which removes suspected closures in zip codes where the number of active hospitals does not decrease the year after the suspected closure, is a helpful starting point. However, it has some limitations.

As mentioned in section2.3, the first-pass method ignores the case where both the closure year and the following year have zero active hospitals, and it incorrectly categorize this situation as a potential merger. To solve this, more considerations are needed like introducing a rule for cases where both the closure year and the following year have zero active hospitals: check if the area had limited healthcare resources. If so, classify it as a true closure rather than a potential merger.

Hospital data might not be fully updated in the following year. Some hospitals may actually be closed while having a late report, leading to an unchanged active hospitals number. There are also cases where hospitals relocate like moving in to or out from a particular zip code area while keeping the same total number. Consolidating and seperating could also happen at the same time while the total number of active hospitals might remain unchanged.

Ways to do better can be tracking hospitals for two to three more years before confirming a closure. This allows enough time for data to be updated and corrected, giving a more accurate picture of real changes. Cross checking CMS and specific address or geographic coordinates rather than just counting hospitals in the zip code or use CMS seperately. By observing exact locations and CMS at same time, we could identify hospitals that have moved or merged.


Partner_2:   

ZIP codes with at least one hospital closure between 2016 and 2019 are the most directly impacted outcomes. Here we begin by computationally ignoring follow-up in subsequent years of closure, and also defaulting to the idea that hospital closures within a given ZIP code mean fewer visits for residents.  

The current methodology has limitations, such as the fact that distance effectively ignores the population density of the corresponding area in English: zip code areas vary widely in size and population density. Some areas may have a large number of residents, while others are relatively empty. Using zip codes alone to delineate areas may mask the actual distribution of health care needs and resources.  

Improvements can be made by considering a combination of factors: for example, when assessing accessibility to hospitals, taking into account the distribution of populations and residential areas, transportation networks, and public transportation options (which are often included in geographic data as well) can provide a more complete picture of how residents actually travel to hospitals.  